Adaptation--In Piaget's Theory of Development, there are two cognitive processes that are crucial for progressing from stage to stage: assimilation, accommodation. Taken together, assimilation and accomodation make up adaptation, which refers to the child's ability to adapt to his or her environment. Siegler, R. (1991). Children's Thinking. Englewood Cliffs, NJ: Prentice-Hall. Vasta, R., Haith, M. M., & Miller, S. A. (1995). Child Psychology: The Modern Science. New York, NY: Wiley. 
Assimilation--This refers to the way in which a child transforms new information so that it makes sense within their existing knowledge base. That is, a child tries to understand new knowledge in terms of their existing knowledge. For example, a baby who is given a new knowledge may grasp or suck on that object in the same way that he or she grasped or sucked other objects.
Accomodation--This happens when a child changes his or her cognitive structure in an attempt to understand new information. For example, the child learns to grasp a new object in a different way, or learns that the new object should not be sucked. In that way, the child has adapted his or her way of thinking to a new experience.
Alzheimer's Disease--Alzheimer's Disease (AD), a term coined by Alois Alzheimer in 1907, is a relentlessly progressive disease characterized by cognitive decline, behavioural disturbances, and changes in personality. Current estimates of prevalence of AD in Canada suggest that 5.1% of all Canadians 65 and over meet the criteria for the clinical diagnosis of AD, which translates into approximately 161,000 cases. AD prevalence is slightly higher in women than in men. It may be that this difference is due to the longer life expectancy of women although other factors have not been ruled out. The prevalence of dementia is strongly associated with age, affecting 1% of the Canadian population aged 65 to 74, 6.9% of individuals 75-84 and 26% of individuals 85 years and older (Canadian Study of Health and Aging, 1994). The diagnostic criteria for dementia of the Alzheimer's Type (DAT) are as follows: A. The development of multiple cognitive deficits manifested by both: Memory impairment (impaired ability to learn new information or to recall previously learned information) One or more of the following cognitive disturbances: aphasia (language disturbance) apraxia (impaired ability to carry out motor activities despite intact motor function) agnosia (failure to recognize or identify objects despite intact sensory function) disturbances in executive functioning (i.e., planning, organizing, sequencing, abstracting) B. The cognitive deficits in Criteria A1 and A2 each cause significant impairment in social and occupational functioning and represent a significant decline from a previous level of functioning. C. The course is characterized by gradual onset and continuing cognitive decline D. The cognitive deficits in Criteria A1 and A2 are not due to any of the following: (1) other central nervous system conditions that cause progressive deficits in memory and cognition (e.g., cerebrovascular disease, Parkinson's Disease, Huntington's Disease, subdural hematoma, normal pressure hydrocephalus, brain tumor). (2) systemic conditions that are known to cause a dementia (e.g., hypothyroidism, vitamin B12 or folic acid deficiency, hypercalcemia, neurosyphilis, HIV infection) (3) substance-induced conditions E. The deficits do not occur exclusively during the course of a delirium F. The disturbance is not better accounted for by another Axis 1 disorder (e.g., Major Depressive Disorder, Schizophrenia) American Psychiatric Association: Diagnostic and Statistical Manual of Mental Disorders, Fourth Edition. Washington, D.C, American Psychiatric Association, 1994. The diagnosis of AD is based on exclusionary criteria (i.e., the absence of an identifiable cause) with diagnosis confirmed at autopsy. Treatment strategies to date have been largely ineffective, with experimental treatments mainly directed toward overcoming the cholinergic deficit. Canadian Study of Health and Aging: study methods and prevalence of dementia. Canadian Medical Association Journal, 1994: 150(6). (p> Whitehouse, P.J. Dementia. Philadelphia: F.A. Davis Company 
Apparent Motion--This is a perceptual phenomenon that occurs when we perceive motion in two or more static images that are presented in succession with appropriate spatial and temporal displacements. The ability to perceive this phenomenon is mediated by the visuospatial pathway of the visual association regions of the brain. We see examples of this phenomenon almost everyday when we view television or movies. This is an example of a cognitively impenetrable perception. That is, even though we know that the images are not moving, we still perceive motion. Marr, D. (1982). Vision, Freeman: San Francisco, pp.159-182. Zeki, S. (1992). The Visual Image in Mind & Brain. Scientific American, 241(3), 150-162. 
Articulatory Loop--The articulatory loop [AL] is one of two passive slave systems within Baddeley's (1986) tripartite model of working memory. The AL, responsible for storing speech based information, is comprised of two components. The first component is a phonological memory store which can hold traces of acoustic or speech based material. Material in this short term store lasts about two seconds unless it is maintained through the use of the second subcomponent, articulatory subvocal rehearsal. Prevention of articulatory rehearsal results in very rapid forgetting. Try this experiment with a friend. Present your friend with three consonants (e.g., C-X-Q) and ask them to recall the consonants after a 10 second delay. During the 10 second interval, prevent your friend from rehearsing the consonants by having them count 'backwards by threes' starting at 100. You will find that your friend's recall is significantly impaired! See Murdoch (1961) and Baddeley (1986) for a complete review. Baddeley, A. (1986). Working memory. Oxford: Clarendon Press. Murdock, B.B. Jr. (1961). The retention of individual items. Journal of Experimental Psychology, 62, 618-625. 
Artificial Intelligence--Artificial intelligence is concerned with the attempt to develop complex computer programs that will be capable of performing difficult cognitive tasks. Some of those who work in artificial intelligence are relatively unconcerned as to whether the programs they devise mimic human cognitive functioning, while others have the explicit goal of simulating human cognition on the computer. The artificial intelligence approach has been applied to several different areas within cognitive psychology, including perception, memory, imagery, thinking, and problem solving. There are a number of advantages of the artificial intelligence approach to cognition. Computer programming requires that every process be specified in detail, unlike cognitive psychology which often relies on vague descriptions. AI also tends to be highly theoretical, which leads to general theoretical orientations having wide applicability. The main disadvantage of AI is that there is a lot of controversy about the ultimate similarity between human cognitive functioning and computer functioning. Some of the major differences between brains and computers were spelled out in the following terms by Churchland (1989, p.100): "The brain seems to be a computer with a radically different style. For example, the brain changes as it learns, it appears to store and process information in the same places...Most obviously, the brain is a parallel machine, in which many interactions occur at the same time in many different channels." This contrasts with most computer functions which involves serial processing and relatively few interactions. Churchland, P.S. (1989). From Descartes to neural networks. Scientific American, July, 100. Eysenck, M.W. ed. (1990). The Blackwell Dictionary of Cognitive Psychology. Cambridge, Massachusetts: Basil Blackwell Ltd. 
Attention--"Attention" is a term commonly used in education, psychiatry and psychology. The definition is often vague. Attention can be defined as an internal cognitive process by which one actively selects environmental information (ie. sensation) or actively processes information from internal sources (ie. visceral cues or other thought processes). In more general terms, attention can be defined as an ability to focus and maintain interest in a given task or idea, including managing distractions. William James, a 19th century psychologist explains attention as follows: "Everyone knows what attention is. It is the taking possession by the mind in clear and vivid form, of one out of what seem several simultaneously possible objects or trains of thought...It implies withdrawl from some things in order to deal effectively with others, and is a condition which has a real opposite in the confused, dazed, scatterbrained state." (p. 403) Attention is important to psychologists because it is often considered a core cognitive process, a basis on which to study other cognitive processes; most importantly learning. DeGangi and Porges (1990) illustrate only "when a person is actively engaged in voluntary attention, functional purposeful activity and learning can occur." (p. 6) Poor attention is often a key symptom of behaviour disorders such as hyperactivity and learning disorders. DeGangi, Georgia and Porges, Stephen. (1990). Neuroscience Foundations of Human Performance. Rockville, MD: American Occupational Therapy Association Inc. James, William. (1890). Principles of psychology. New York: Holt. 
Attention Getting--Attention getting is more than just the orienting reflex, it is the "initial orientation or alerting to a stimulus." Though this may be considered an automatic act, in fact it requires complex active thought processing. Attention getting is reliant on the qualitative nature of the stimulus. The stimulus must be stong enough to elicit a response. DeGangi and Porges (1990) explain the types of stimuli that are attention getting vary according to past experiences of the individual, what they already know, individual reactivity to sensory stimuli, and what an individual has determined to be important to them. A hungry person may be more apt to pay attention to the smell of food than the sounds surrounding them in a traffic jam! Attention getting is important to psychologists, particularily developmental psychologists because of its role in learning. A child's chosen attention getting stimuli can guide his/her learning abilities. "A child who learns better through the auditory channel will orient more readily to a song about body parts than a picture of a body." DeGangi, Georgia and Porges, Stephen. (1990). Neuroscience Foundations of Human Performance. Rockville, MD: American Occupational Therapy Association Inc. 
Attention Holding--Attention holding is the "maintenance of attention when a stimulus is intricate or novel." Stimuli that hold our attention must be both novel and complex in order to encourage information processing. Attention holding is measured by how long one engages in a cognitive activity involving that stimulus. Attention holding is important because of its role in learning. If an activity or stimulus is moderately complex, the person will expend energy in information processing. In other words, the person will expend energy in learning. Unfortunately, this can be complicated by poor motivation. Low motivation may present a challenge as the psychologist (or other professional) must determine if the decreased motivation is due to sensory processing problems, cognitive impairment, or other learning-related problems (of which poor attention holding may be identified). DeGangi, Georgia and Porges, Stephen. (1990). Neuroscience Foundations of Human Performance. Rockville, MD: American Occupational Therapy Association Inc. 
Attention Releasing--Attention releasing is the final stage in DeGangi and Porges' (1990) process of sustained attention. Attention releasing can simply be defined as the "releasing or turning off of attention from a stimulus." Attention releasing can occur for a variety of reasons. A person can fatigue physically or mentally requiring release of attention. Arousal level can decrease, therefore a different type/strength of stimuli becomes required to maintain an alert and active state. Attention releasing provides a person with a method to reach closure on a given activity, task, or event thereby allowing that person to switch attention to something new. As with attention getting and holding, attention releasing (the ability to shift focus) plays an important role in the learning process. DeGangi, Georgia and Porges, Stephen. (1990). Neuroscience Foundations of Human Performance. Rockville, MD: American Occupational Therapy Association Inc. 
Analogy--In cognitive psychology, analogy is considered an important method of problem solving. The problem solver attempts to use his or her knolwedge of one problem to solve another problem about which she or he has very little or no information. Barsalou (1992) provides the following example of problem solving by analogy: "...someone who has worked at the complex for a while could simply explain to you that the layout is analogous to a starfish. On hearing this analogy you might transfer knowledge about starfish to the office complex. Thus the knowledge that a starfish has a circular body, with five legs extending from it radially and symetrically would lead to the belief that the office complex contains a center circular body, with five tapered buildings extending from it in a radially symmetric pattern." (p.110) Obviously people do not use all of their knowledge about one problem to solve another problem. In the context of his starfish example Barsalou points out that we would not begin to think that the office complex is alive, or that it lives underwater. One problem facing cogntive psychologists is to determine how people decide upon the extent to which an analogy applies. Determining how this may be done is more difficult than it may seem. Consider that, given enough time people can find analogies between any two phenomena. We might want to say that, like the starfish, the office complex is alive--its heating ducts are like blood vessels, its doors are like mouths eating the people who enter the office complex every day. As a cognitive process analogy seems limitless. In a science that strives for regularity and lawfulness the limitlessness of analogical thinking poses a serious problem. Barsalou, L. (1992). Cognitive Psychology: An Overview for Cognitive Psychologists. Lawrence Erlbaum Associates. Hillsdale: New Jersey. 
Behavioural Indeterminacy--The claim that in principle psychology is restricted to establishing weak equivalence . Weak equivalence is equivalence with respect to input/output behaviour. Therefore, measuring behavioural data is unable to establish equivalence at the level of functional architecture . Behavioural studies are indeterminate with respect to strong equivalence. This issue is of importance to cognitive psychology because, if true, it implies that cognitive psychology cannot generate insight into cognition without importing knowledge based on non-behavioural observations from other disciplines. Pylyshyn, Z. W., "Computing in cognitive science", in Posner, M.I. (ed.) 1989, Foundations of Cognitive Science, MIT Press, Cambridge MA. 
Bottom-Up Processing--The cognitive system is organized hierarchically. The most basic perceptual systems are located at the bottom of the hierarchy, and the most complex cogntive (e.g. memory, problem solving) systems are located at the top of the hierarchy. Information can flow both from the bottom of the system to the top of the system and from the top of the system to the bottom of the system. When information flows from the bottom of the sytstem to the top of the system this is called "bottom-up" processing. Lower level systems categorize and describe incoming perceptual information and pass this descriptive information onto higher levels for more complex processing.
Cognitive Development (In Children)--Generally it is referred to the changes which occur to a person's cognitive structures, abilities, and processes. The most widely known theory of childhood cognitive development was proposed by Jean Piaget in 1969. He proposed the idea that cognitive development consisted of the development of logical competence, and that the development of this competence consists of four major stages (sensori-motor, preoperational, concrete operational, and formal operational). He also argued that a child's cognitive performance depended more on the stage of development he was in than on the specific task being performed. More recent studies have cast some doubt on Piaget's theory of homogeneous performance within a given stage. Instead, it is now believed that performance varies greatly within each stage and depends more on the acquisition and development of language, perception, decision rules, and real-world knowledge for any individual child. 
Cognitive Penetrability--An approach to testing strong equivalence . The cognitive penetrability approach seeks to establish whether phenomena are equivalent at the level of functional architecture by investigating whether phenomena are independent of beliefs and goals, that is if they are primitive. If manipulation of beliefs and goals systematically alters the empirical phenomenon then the phenomenon is not describing functional architecture and is cognitively penetrable. The cognitive penetrability approach was used in the imagary debate in cognitive science in the 1980's. Pylyshyn, Z. W., "Computing in cognitive science", in Posner, M.I. (ed.) 1989, Foundations of Cognitive Science, MIT Press, Cambridge MA. 
Cognitive Science--Definition 1 "the study of intelligence and intelligent systems, with particular reference to intelligent behaviour as computation" (Simon & Kaplan, 1989) Simon, H. A. & C. A. Kaplan, "Foundations of cognitive science", in Posner, M.I. (ed.) 1989, Foundations of Cognitive Science, MIT Press, Cambridge MA. Definition 2 Cognitive science refers to the interdisciplinary study of the acquisition and use of knowledge. It includes as contributing disciplines: artificial intelligence, psychology, linguistics, philosophy, anthropology, neuroscience, and education. The cognitive science movement is far reaching and diverse, containing within it several viewpoints. Cognitive science grew out of three developments: the invention of computers and the attempts to design programs that could do the kinds of tasks that humans do; the development of information processing psychology where the goal was to specify the internal processing involved in perception, language, memory, and thought; and the development of the theory of generative grammar and related offshoots in linguistics. Cognitive science was a synthesis concerned with the kinds of knowledge that underlie human cognition, the details of human cognitive processing, and the computational modeling of those processes. There are five major topic areas in cognitive science: knowledge representation, language, learning, thinking, and perception. Eysenck, M.W. ed. (1990). The Blackwell Dictionary of Cognitive Psychology. Cambridge, Massachusetts: Basil Blackwell Ltd. Definition 3 Generally stated, this is the study of intelligence and intelligence systems. It is a relatively new science that combines knowledge gained from a number of disciplines. These include: computer science,neuroscience, cognitive psychology, philosophy, and linguistics. As a result of the collaborative effort between these disciplines, there have been, and will continue to be, huge advancements in our understanding of human cognition. 
Central Executive--The central executive, the most important yet least well understood component of Baddeley's (1986) working memory model, is postulated to be responsible for the selection, initiation, and termination of processing routines (e.g., encoding, storing, retrieving). Baddeley (1986, 1990) equates the central executive with the supervisory attentional system (SAS) described by Norman and Shallice (1980) and by Shallice (1982). According to Shallice (1982), the supervisory attentional system is a limited capacity system and is used for a variety of purposes, including:    tasks involving planning or decision making    trouble shooting in situations in which the automatic processes appear to be running into difficulty    novel situations    dangerous or technically difficult situations    situations where strong habitual responses or temptations are involved Extensive damage to the frontal lobes may result in impairments in central executive functioning. Baddeley (1986) coined the term dysexecutive syndrome (DES) to describe dysfunctions of the central executive The classic frontal syndrome is characterized by "disturbed attention, increased distractibility, a difficultly in grasping the whole of a complicated state of affairs...well able to work along old routines...(but)...cannot learn to master new types of task, in new situations...[the patient is]...at a loss' (Rylander, 1939, p.20). In other words, patients suffering from frontal lobe syndrome lack flexibility and the ability to control their processing resources, functions attributed to the central executive. Baddeley, A.D. (1990). Human memory: Theory and practice. Oxford, Oxford University Press. Baddeley, A.D. (1986). Working memory. Oxford: Clarendon Press. Norman, D.A., & Shallice, T. (1980). Attention to action. Willed and automatic control of behavior. University of California San Diego CHIP Report 99. Shallice, T. (1982). Specific impairments of planning. Philosophical Transactions of the Royal Society London B 298, 199-209. Rylander, G. (1939). Personality changes after operations on the frontal lobes. Acta Psychiatrica Neurologica, Supplement No. 30. 
Cascade Processing--Under the assumption that a cpmplex task can be broken down into distinct stages of information processing, and that these stages can be sequentially ordered, the complex task can be performed by completing each distinct stage. Unlike discrete processing, with cascade models the latter stages of information processing can begin operating before the completion of earlier information processing stages. Connectionist models of information processing operate in a cascade manner and are important for the way in which these models can learn relationships between stimule and responses. Depending on the complexity of the information being processed, it may be transmitted between some processing stages in a cascade manner, but in other stages it may be processed in a discrete manner. Blackwell's Dictionary of Cognitive Science 
Crystallized Intelligence--Crystallized intelligence can be defined as "the extent to which a person has absorbed the content of culture."(Belsky, 1990, p. 125) It is the store of knowledge or information that a given society has accumulated over time. Crystallized intelligence is measured by most of the verbal subtests of the Wechsler Adult Intelligence Scale (WAIS). Crystallized intelligence is important to psychologists as it relates to the study of aging. There is ongoing intense debate among psychologists as to whether or not intelligence declines with aging. Horn (1970) hypothesized that because crystallized intelligence is based on learning and experience, it remains relatively stable over time. He claims it may even increase "as the rate at which we acquire or learn new information in the course of living balances out or exceeds the rate at which we forget." (as quoted in Belsky, 1990, p. 125) On the other side of the debate, Belsky (1990) claims crystallized intelligence in fact declines with age. Why? Because, "at a certain time of life the cumulative effect of losses - of job, of health, of relationships - cause disengagement from the culture, and so forgetting finally exceeds the rate at which knowledge is acquired." (p. 125) Belsky, Janet K. (1990). The Psychology of Aging Theory, Research, and Interventions. Pacific Grove: Brooks/Cole Publishing Company. Horn, Jack (1970) "Organization of data on life-span development of human abilities" in Life- span developmental psychology: Research and Theory. R. Goulet and P.B. Baltes (eds.). New York: Academic Press. Horn, Jack (1976) "On the myth of intellectual decline during adulthood" in American Psychologist. 
Cued Recall--This is a component of a memory task in which the subject is asked to recall items that were presented to them on an intial training, or initial presentation list. However, it is slightly different than the free recall task because the subject is given a hint, or a cue, about the items on the original list. For example, and experimenter may say: "Tell me all the words from the list that were animals". 
Deductive (Logical) Inference--Inferences are made when a person (or machine) goes beyond available evidence to form a conclusion. With a deductive inference, this conclusion always follows the stated premises. In other words, if the premises are true, then the conclusion is valid. Studies of human efficiency in deductive inference involves conditional reasoning problems which follow the "if A, then B" format. The task of making deductions consists of three stages. First, a person must understand the meaning of the premises. Next they must be able to formulate a valid conclusion. Thirdly, a person should evaluate their conclusion to tests its validity. Although deductive inference is easy to test or model, the results of this type of inference never increase the semantic information above what is already stated in the premises. Blackwell's Dictionary of Cognitive Science. Johnson-Laird, Philip N., Human and Machine Thinking, 1993. 
Dementia--Dementia is a clinical state characterized by loss of function in multiple cognitive domains. The most commonly used criteria for diagnoses of dementia is the DSM-IV (Diagnostic and Statistical Manual for Mental Disorders, American Psychiatric Association). Diagnostic features include : memory impairment and at least one of the following: aphasia, apraxia, agnosia, disturbances in executive functioning. In addition, the cognitive impairments must be severe enough to cause impairment in social and occupational functioning. Importantly, the decline must represent a decline from a previously higher level of functioning. Finally, the diagnosis of dementia should NOT be made if the cognitive deficits occur exclusively during the course of a delirium. There are many different types of dementia (approximately 70 to 80). Some of the major disorders causing dementia are: Degenerative diseases (e.g., Alzheimer's Disease, Pick's Disease) Vascular Dementia (e.g., Multi-infarct Dementia) Anoxic Dementia (e.g., Cardiac Arrest) Traumatic Dementia (e.g., Dementia pugilistica [boxer's dementia]) Infectious Dementia (e.g., Creutzfeldt-Jakob Disease) Toxic Dementia (e.g., Alcoholic Dementia) 7.9 % of all Canadians 65 years and older meet the criteria for the clinical diagnoses of dementia (Canadian Study on Health and Aging, 1994). Alzheimer's Disease is the major cause of dementia, accounting for 64% of all dementias in Canada for persons 65 and older and 75% of all dementias for persons 85 plus. American Psychiatric Association: Diagnostic and Statistical Manual of Mental Disorders, Fourth Edition. Washington, D.C, American Psychiatric Association, 1994. Canadian Study of Health and Aging: study methods and prevalence of dementia. Canadian Medical Association Journal, 1994: 150(6). 
Discrete Processing--A model using discrete processing requires that information is passed from one stage to another only after the processing in the first stage is complete. Therefore, the processing time required in a discrete model is additive and equal to the sum of the time taken at each level of processing. The advantage of this type of model is that it provides a convienent method of understanding the effects of different variables on the performance of a given task. Blackwell's Dictionary of Cognitive Science 
Elaborative Rehearsal--Elaborative rehearsal is a type of rehearsal proposed by Craik and Lockhart (1972) in their Levels of Processing model of memory. In contrast to maintenance rehearsal, which involves simple rote repetition, elaborative rehearsal involves deep sematic processing of a to-be-remembered item resulting in the production of durable memories. For example, if you were presented with a list of digits for later recall (4968214), grouping the digits together to form a phone number transforms the stimuli from a meaningless string of digits to something that has meaning. Craik, F.I.M., & Lockhart, R.S. (1972). Levels of processing. A framework for memory research. Journal of Verbal Learning and Verbal Behaviour, 11, 671-684. 
Enactment--Weick (1988) describes the term enactment as representing the notion that when people act they bring structures and events into existence and set them in action. The process of enactment involves two steps. First, preconceptions are used to set aside portions of the field of experience for further attention, that is, perception is focused on predetermined stimuli. Second, people act within the context of these portions of experience guided by preconceptions in such a way as to reinforce these preconceptions. Hence, attention to certain stimuli will guide subsequent action so that those stimuli are confirmed as important. The result of the process of enactment is the enacted environment (Weick, 1988). This enacted environment comprises "real" objects but the significance, meaning and content of these objects will vary. These objects are not significant unless they are acted upon and incorporated into events, situations and explanations. In this way the enacted environment is a direct result of the preconceptions held by the social actor. An enacted environment is internalised by social actors as the way in which actions have led to certain consequences; it is therefore analogous to the concept of schema and is the source of expectations for future action (Weick, 1988) . An enacted environment is "a map of if-then assertions in which actions are related outcomes" that in turn serve as expectations for future action and focus perception in such way that these preconceived relationships will be supported. The importance of the notion of enactment is that it provides a direct link between individual cognitive processes and environments. By showing how preconceptions can shape the nature of the environment this concept allows one to argue the importance of schema in the sensemaking process. Schema guide both perception and inference (Fiske & Taylor, 1991) and so will 'enact' environment by assigning significance, meaning and content to objects perceived in the environment. Fiske, S. T., & Taylor, S. E. (1991). Social Cognition (2nd ed.). New York: McGraw-Hill. Weick, K. E. (1988). Enacted sensemaking in crisis situations. Journal of Management Studies, 24(4). 
Encoding--Encoding refers to the processess of how items are placed into memory. 
Equilibration--According to Piaget, development is driven by the process of equilibration. Equilibration encompasses assimilation (i.e., people transform incoming information so that it fits within their existing thinking) and accommodation (i.e, people adapt their thinking to incoming information). Piaget suggested that equilibration takes place in three phases. First children are satisfied with their mode of thought and therefore are in a state of equilibrium. Then, they become aware of the shortcomings in their existing thinking and are dissatisfied (i.e., are in a state of disequilibration and experience cognitive conflict). Last, they adopt a more sophisticated mode of thought that eliminates the shortcomings of the old one (i.e., reach a more stable equilibrium). 
Error Analysis--One of the key goals of cognitive science is to develop theories that are strongly equivalent with respect to to-be-explained systems. This requires that evidence be collected to defend the claim that the model and the to-be-explained system are carrying out the same procedures to compute a function. One kind of information that could be used to examine this claim is called error analysis. In an error analysis, one could (for two different systems) rank order problems in terms of their difficulty, as revealed by their likelihood to produce mistakes. This is an example of relative complexity evidence. A more detailed approach would be to classify the nature of the errors that each system made. In either case, if the two systems were strongly equivalent, then we would expect them to produce the same rank orderings of difficulty, and to also produce the same qualitative patterns of errors. Pylyshyn, Z.W. (1984). Computation and cognition. Cambridge, MA: MIT Press. 
Fluid Intelligence--Fluid intelligence is tied to biology. It is defined as our "on-the-spot reasoning ability, a skill not basically dependant on our experience." (Belsky, 1990, p. 125) Belsky (1990) indicates this type of intelligence is active when the central nervous system (CNS) is at its physiological peak. Fluid intelligence is measured by the performance subtasks on the Wechsler Adult Intelligence Scale (WAIS). Fluid intelligence is important to psychologists as it relates to the study of aging. There is ongoing intense debate among psychologists as to whether or not intelligence declines with aging. Belsky (1990) claims fluid intelligence "reaches a peak in early adulthood and then regularly declines." (p. 125) This is because of the physiological changes that accompany aging. "The development of CNS structures is exceeded by the rate of CNS breakdown." (Horn, 1970 as quoted in Belsky, 1990, p. 125) Belsky, Janet K. (1990). The Psychology of Aging Theory, Research, and Interventions. Pacific Grove: Brooks/Cole Publishing Company. Horn, Jack (1970) "Organization of data on life-span development of human abilities" in Life- span developmental psychology: Research and Theory. R. Goulet and P.B. Baltes (eds.). New York: Academic Press. Horn, Jack (1976) "On the myth of intellectual decline during adulthood" in American Psychologist. 
Free Recall--Free recall is a basic paradigm used to study human memory. In a free recall task, a subject is presented a list of to-be-remembered items, one at at time. For example, an experimenter might read a list of 20 words aloud, presenting a new word to the subject every 4 seconds. At the end of the presentation of the list, the subject is asked to recall the items (e.g., by writing down as many items from the list as possible). It is called a free recall task because the subject is free to recall the items in any order that he or she desires. The free recall task is of interest to cognitive science because it provided some of the basic information used to decompose the mental state term "memory" into simpler subfunctions ("primary memory", "secondary memory"). This is because the results of a free recall task were typically plotted as a serial position curve. This curve exhibited a recency effect and a primacy effect. The behavior of these two effects provided support to the hypothesis that the free recall task called upon both a short-term and a long-term memory. 
Functional Analysis--Functional analysis is a methodology that is used to explain the workings of a complex system. The basic idea is that the system is viewed as computing a function (or, more generally, as solving an information processing problem). Functional analysis assumes that such processing can be explained by decomposing this complex function into a set of simpler functions that are computed by an organized system of subprocessors. The hope is that when this type of decomposition is performed, the subfunctions that are defined will be simpler than the original function, and as a result will be easier to explain. A very detailed treatment of functional analysis is provided by Cummins (1983). He proposes a three-stage methodology that defines functional analysis. In the first stage, the to-be-explained function is defined. In the second stage, analysis is performed. The to-be-explained function is decomposed into an organized set of simpler functions. This analysis can proceed recursively by decomposing some (or all) of the subfunctions into sub-subfunctions. In the third stage, analysis is stopped by subsuming the bottom level of functions. This means that the operation of each of these operation is explained by appealing to natural laws (e.g., mechanical or biological principles). If functional analysis is applied to an information processing system, then the level of subsumed functions defines the functional architecture for that information processor. Functional analysis is important to cognitive science because it offers a natural methodology for explaining how information processing is being carried out. For instance, any "black box diagram" offered as a model or theory by a cogntive psychologist represents the result of carrying out the analytic stage of functional analysis. Any proposal about what constitutes the cognitive architecture can be viewed as a hypothesis about the nature of cognitive functions at the level at which these functions are subsumed. Cummins, R. (1983). The nature of psychological explanation. Cambridge, MA: MIT Press. 
Functional Architecture--The functional architecture can be viewed as the set of basic information processing capabilities available to an information processing system. "Specifying the functional architecture of a system is like providing a manual that defines some programming language. Indeed, defining a programming language is equivalent to specifying the functional architecture of a virtual machine" (Pylyshyn, 1984, p. 92). In other words, if it is assumed that cognition is the result of the brain's "running of a program", then the functional architecture is the language in which that program has been written. The functional architecture is of interest to cognitive science because if offers an escape from Ryle's Regress (a.k.a. the homunculus problem). The functional architecture is comprised of a set of primitive operations or functions. This means that these basic functions cannot be explained by being further decomposed into less complex ("smaller") subfunctions. Instead, they must be explained by appealing to implementational properties (e.g., for human cognition, properties of the human brain). As a result, the functional architecture represents the point at which the decomposition of mental state terms into other mental state terms via functional analysis can stop. By specifying the functional architecture, one converts the black box descriptions that cognitivists create into explanations. Pylyshyn, Z.W. (1984). Computation and cognition. Cambridge, MA: MIT Press. 
Generalization--Klahr & Wallace (1982) felt that Piaget's theory of adaptation was not enough to explain cognitive development. They therefore developed a new theory, and posited that the mechanism behind development was generalization. Klahr and Wallace divided generalization into three more specific categories: the time line, regularity detection, and redundancy elimination (Siegler, 1991). Klahr and Wallace have developed a self-modifying computer simulation that models findings about children's thinking, and can demonstrate these processes in generalization. Klahr, D. (1982). Nonmonotone assessment of monotone development: An information processing analysis. In S. Strauss (Ed.), U-shaped behavioral growth. New York: Academic Press. Siegler, R. (1991). Children's Thinking. Englewood Cliffs, NJ: Prentice-Hall. Vasta, R., Haith, M. M., & Miller, S. A. (1995). Child Psychology: The Modern Science. New York, NY: Wiley. 
Time Line--The time line contains the data on which generalizations are based. In Klahr and Wallace's theory, whenever a system encounters a situation, it records the responses to that situation, the outcomes from those actions, and what new situations arose as a result. This recording of events ensures that the system keeps all the information about an even stored so that it can be referred back to in the future.
Regularity Detection--This process uses the contents of the time line to draw generalizations about experience. The system notes situations that are similar and notes where variations do not change the outcomes of situations.
Redundancy Elimination--This process improves efficiency by identifying processeing steps that are unecessary. In this way, it reaches a generalization that a less-complex sequence can achieve the same goal (Siegler, 1991).
Humor--There are many reasons why people find something humorous, which are reflected in the large number of theories on the subject. Humor has been related to aggression, incongruity, and surprise. The cognitive psychologist's interest in the subject is usually related to the notion that humor stems from a resolution of incongruity. For example, consider this joke by W.C. Field. "Do you believe in clubs for children?" "Only when kindness fails". Schultz(1974) offered a three step theory of processing. In the first stage, the listener notices the incorrect interpretation of the ambiguous element (clubs = social groups). In the second step, the incorrect element of incongruity is processed ( "only when kindness fails"). In the final stage the hidden meaning of the ambiguous element is perceived (clubs = sticks). The incongruity resolution theory explains the fact that a joke previously encountered will seem less funny on subsequent exposure. Similarly, Freud (1905, in Minsky 1985) suggested that humorous stories are a way of fooling our internal censors. A joke's power comes from a description that fits two different frames at once. The first meaning must be transparent and innocent, while the second meaning is disguised and reprehensible. Although most cognitive psychologists have not extended their theorizing to humor, it does have an important cognitive aspect. In particular, cognitive theory helps provide an explanation of why verbal jokes are found amusing by looking at the comprehension processes involved. Kristal, L. ed. (1981). ABC of Psychology. London: Multimedia Publications Inc. Minsky, M. (1985). The Society of Mind. New York, NY: Simon & Schuster, Inc. Schultz, T.R. (1974). Order and processing in humor appreciation. Canadian Journal of Psychology. 28. p. 409-420. 
Hebbian Learning Rule--The Hebbian Learning Rule is a learning rule that specifies how much the weight of the connection between two units should be increased or decreased in proportion to the product of their activation. The rule builds on Hebbs's 1949 learning rule which states that the connections between two neurons might be strengthened if the neurons fire simultaneously. The Hebbian Rule works well as long as all the input patterns are orthogonal or uncorrelated. The requirement of orthogonality places serious limitations on the Hebbian Learning Rule. A more powerful learning rule is the delta rule, which utilizes the discrepancy between the desired and actual output of each output unit to change the weights feeding into it. Bechtel, W., & Abrahamsen, A. (1993). Connectionism and the mind. Oxford, UK: Blackwell. Hebb, D.O. (1949) The organization of behavior. New York: Wiley. Rumelhart, D.E., & McClelland, J. L. and the PDP Research Group (1986). Parallel Distributed Processing: Explorations in the microstructure of cognition, vol. 1: Foundations, Cambridge, MA: MIT Press/Bradford Books. 
Incidental Learning Paradigm--The incidental learning paradigm is an experimental paradigm used to investigate learning without intent. Using this paradigm, several groups of subjects are presented with the same list of items (e.g., 20 words) and are instructed to process them in different ways (different orienting conditions), with each group asked to perform a different activity or orienting task with the list. For example, count the number of letters in each word (shallow processing); name a rhyming word for each item (again, shallow processing, but deeper than #1; form an image of each word and rate the vividness of each image (deep processing). Importantly, subjects are not told that there will be a subsequent test of memory. At the end of the list presentation, subjects are unexpectedly asked to recall as many of the words as possible. Processing information at a deeper level results in superior recall of that information (Eysenck, 1974). 
Inductive (Pragmatic) Inference--Inferences are made when a person (or machine) goes beyond available evidence to form a conclusion. An inductive inference is one which is likely to be true because of the state of the world. Unlike deductive inferences, inductive inferences do yield consclusions that increase the semantic information over and above that found in the initial premises. However, in the case of inductive inferences, we cannot be sure that our conclusion is a logical result of the premises, but we may be able to assign a likelihood to each conclusion. Similar to deductive inference, induction can be broken down into three stages. The first stage is to understand the observation or stated information. The second is to form a hypothesis that attempts to describe the above information in relation to t person's general knowledge. The resulting conclusion goes beyond initial information by incorporating one's general knowledge in the result. The third step is to evaluate the validity of the conclusion that was reached. Blackwell's Dictionary of Cognitive Science Johnson-Laird, Philip N., Human and Machine Thinking, 1993. 
Intermediate State Evidence--One of the key goals of cognitive science is to develop theories that are strongly equivalent with respect to to-be-explained systems. This requires that evidence be collected to defend the claim that the model and the to-be-explained system are carrying out the same procedures to compute a function. One type of evidence that can be used to support this claim is intermediate state evidence. This involves observations of the intermediate steps, and/or the intermediate states of knowledge, that the two systems pass through as they move from being given a problem to providing an answer. For example, if one was using a Turing machine as a model, then an immediate source of intermediate state evidence would be what the machine does to its tape with each processing step. In studying human subjects, intermediate state evidence is not directly available. However, one method that might provide some evidence about these intermediate states is protocol analysis. Pylyshyn, Z.W. (1984). Computation and cognition. Cambridge, MA: MIT Press. 
Intrusion Errors--In a recall portion of a memory task, these are errors that occur when the subject includes items that were not on the original list. 
Learning Rule--Learning rules, for a connectionist system, are algorithms or equations which govern changes in the weights of the connections in a network. One of the simplest learning procedures for two- layer networks is the Hebbian Learning Rule, which is based on a rule initially proposed by Hebb in 1949. Hebb's rule states that the simultaneous excitation of two neuron results in a strengthening of the connections between them. More powerful learning rules are learning rules which incorporate an error reduction procedure or error correction procedure (e.g., delta rule, generalized delta rule, back propagation). Learning rules incorporating an error reduction procedure utilize the discrepancy between the desired output pattern and an actual output pattern to change (improve) its weights during training. The learning rule is typically applied repeatedly to the same set of training inputs across a large number of epochs or training loops with error gradually reduced across epochs as the weights are fine-tuned. Bechtel, W., & Abrahamsen, A. (1993). Connectionism and the mind. Oxford, UK: Blackwell. Hebb, D.O. (1949) The organization of behavior. New York: Wiley. Rumelhart, D.E., & McClelland, J. L. and the PDP Research Group (1986). Parallel Distributed Processing: Explorations in the microstructure of cognition, vol. 1: Foundations, Cambridge, MA: MIT Press/Bradford Books. 
Levels of Processing --An influential theory of memory proposed by Craik and Lockhart (1972) which rejected the idea of the dual store model of memory. This popular model postulated that characteristics of a memory are determined by it's "location" (ie, fragile memory trace in short term store [STS] and a more durable memory trace in the long term store [LTS]. Instead, Craik and Lockhart proposed that information could be processed in a number of different ways and the durability or strength of the memory trace was a direct function of the depth of processing involved. Moreover, depth of processing was postulated to fall on a shallow to deep continuum. Shallow processing (e.g., processing words based on their phonemic and orthographic components) leads to a fragile memory trace that is susceptible to rapid forgetting. On the other had, deep processing (e.g., semantic or meaning based processing) results in a more durable memory trace. A typical paradigm employed to investigate the Levels of Processing theory is the incidental learning paradigm. Results reveal superior recall for items processed deeply compared to those items processed at the more shallow level (Eysenck, 1974: Hyde & Jenkins, 1969). Craik and Lockhart also distinguished between two kinds of rehearsal, maintenance and elaborative rehearsal. Of the two, elaborative rehearsal is the most effective in producing a more durable memory trace. Craik, F.I.M., & Lockhart, R.S. (1972). Levels of processing. A framework for memory research. Journal of Verbal Learning and Verbal Behaviour, 11, 671-684. Eysenck, M.W. (1974). Age differences in incidental learning. Developmental Psychology, 10, 936-941. Hyde, T.S., & Jenkins, J.J. (1969). Differential effects of incidental tasks on the organization of recall of a list of highly associated words. Journal of Experimental Psychology, 82, 472-481. 
Long-Term Potentiation--"The enduring facilitation of synaptic transmission that occurs following the activation of a synapse by high-frequency stimulation of the presynaptic neuron." (Pinel, 1993, p.515) Originally, this phenomenon was discovered in the Aplysia. However, more recently LTP has been found to occur in mammalian nervous system as well. This was an extremely important finding because it suggested that LTP could be the cellular basis of neural implementation for learning and memory. Long-term potentiation has been found to occur in the mammalian hippocampus. This is an important finding because the hippocampus is believed to be one of the major brain regions responsible for processing memories. LTP is one of the first examples of mechanisms of neural implementation of a cognitive function. This is one of the major goals of cognitive science, and hence is a very important discovery. Pinel, J. (1993). Biopsychology,(2nd Edition) Allyn & Bacon: Toronto. 
Maintenance Rehearsal--Maintenance rehearsal is a type of rehearsal proposed by Craik and Lockhart (1972) in their Levels of Processing Model of memory. Maintenance rehearsal involves rote repetition of an item's auditory representation. In contrast to elaborative rehearsal, this type of rehearsal does not lead to stronger or more durable memories. Craik, F.I.M., & Lockhart, R.S. (1972). Levels of processing. A framework for memory research. Journal of Verbal Learning and Verbal Behaviour, 11, 671-684. 
Mandelbrot Set--A Mandelbrot set is an intricate geometric shape, where if any region of the set is magnified, new and intricate details appear. Every time you focus further on one section, more detail shows up. This will continue ad infinitum, as you investigate further. It was originally postulated to help explain fractals. Another way of looking at this is as follows. When "simple" laws govern systems with large numbers of variables, the underlying order may become obscured by our inability to track every component. Simple rules can produce incredibly complex effects. Mandelbrot sets relate philosophically to the study of cognitive science, in that some theories in the field may need to be more complex in order to be fully validaties, while other topics may be simpler than they first appear. This seems to be the case in the study of groups of agencies and agents in Minsky's The Society of Mind. Cohen J., and I. Stewart (1994). The Collapse of Chaos. New York: Viking Press.
Memory Span--Memory span refers to the number of items (usually words or digits) that a person can hold in working memory. Tests of memory span are often used to measure working memory capacity. A typical test of memory span involves having an examiner read a list of random digits (digit span) or words (word span) aloud at the rate of one per second. At the end of a sequence, subjects are asked to recall the items in order. The average span for normal adults is 7 [see Miller's (1956) magical number seven plus or minus two]. Miller, G.A. (1956). The magical number seven plus or minus two. Some limits on our capacity for processing information. Psychological Review, 63, 81-97. 
Modularity--Jerry Fodor (1983) is the strongest proponent of a modular theory of cognition. Fodor argues that certain psychological processes are self contained--or modular. This is in contrast to "New look" or Modern Cognitivist positions which hold that nearly all psychological processes are interconnected, and freely exchange information. Fodor proposes a three tiered cognitive system. The first level of the system, the "transducer level," transforms environmental signals into a form that can be used by the cognizing organism. The second level, the input systems level, performs basic recognition and description functions. In Fodor's model input systems are modular. The third level of the system, higher level cognitive functions, performs complex operations on the output of the input systems. An example of a higher level process is analogous thinking. Fodor holds that input systems are modular and that higher level cognitive processes are nonmodular. This means that all of the information necessary for performing their tasks of recognition and description is contained within the input systems. For example, object perception might be modular, in which case the object perception module need not reference language modules, or music modules, or mathematics modules in order to perform its operations. In contrast, higher level processes have access to all information contained within the cognitive system when performing a given operation. Fodor provides the example of scientific reasoning (a higher level cognitive process). Potentially, when solving a scientific problem, the scientist can reference any knowledge that he or she has about the world to help in solving this problem. As such, if necessary, knowledge about botany can be referenced in order to understand problems in mathematics. Modular systems have the following properties: 1) They are domain specific--they operate on, and have a computational architecture that is unique to certain stimuli, 2) Their operation is mandatory, or they are cognitively impenetrable--beliefs cannot affect the operations of modules, we cannot help seeing, or hearing the world in a certain way, 3) Modules are fast--modular processes are among the fastest psychological processes,this is because modules are self-contained and need not spend time referencing information outside of the module to complete their tasks, 4) Modules are informationally encapsulated--they need not reference any other psycholgical systems in order to perform their operations, and 5) Modules have shallow outputs--the output of modules is very basic, more complex representations follow after higher level computation. For more information on Fodor's theory of modularity see "The Modularity of Mind," MIT press, 1983. For a precis, and critique of "The Modularity of Mind" see "The Behavioral and Brain Sciences," 1985, Vol. 8, pgs. 1-42. 
Neurocognition--The study of the relationships between neuroscience and cognitive psychology. The goal is to look for specific neurophysiological correlates of cognitive functions. This is based on the assumption that specific brain regions are responsible for mediating certain aspects of cognitive function. Pinel, J. (1993). Biopsychology,(2nd Edition) Allyn & Bacon: Toronto. 
Neuron--These are the specialized, functional cells of the nervous system that conduct neural information. There were originally 2 basic hypotheses about the structure and function of the nervous system: Neuron Hypothesis: "the nervous system is composed of discrete, autonomous cells, or units, that can interact but are not physically connected." (Kolb & Whishaw, 1985, p.317) Nerve Net Hypothesis: "the nervous system is composed of a continuous network of interconnected fibres" (Kolb & Whishaw, 1985, p.317) The current understanding of cognition in the brain represents a combination of these hypotheses. Cognition is viewed as occuring by the interaction between neurons through complex excitatory and inhibitory synapses. As such, cognitive scientists should recognize the need to incorporate basic properties of neurons, and neural organization in the development of models of cognition. The parallel distributed processing model, is a good example of a model that has attempted to account for the basic neural properties. Pinel, J. (1993). Biopsychology,(2nd Edition) Allyn & Bacon: Toronto. Kolb, B., & Whishaw, I. (1985). Fundamentals of Human Neuropsychology (2nd Edition) W.H. Freeman & Co.: New York 
Neuroscience--Neuroscience is the study of the nervous system and has many different branches, such as: Biopsychology, Developmental neurobiology, Neuroanatomy, Neurochemistry, Neuroendocrinology, Neuroethology, Neuropharmacology, Neurophysiology, Neuropsychology. In cognitive science, it is very important to recognize the importance of neuroscience in contributing to our knowledge of human cognition. Cognitive scientists must have, at the very least, a basic understanding of and appreciation for neuroscientific principles. In order to develop accurate models, the basic neurophysiological and neuroanatomical properties must be taken into account. 
Occam's Razor--The simplest definition of Occam's Razor is "Don't make unnecessarily complicated assumptions". It can be used as a philosophical way of sorting the simple theories from the complicated ones. When scientists select theories, they don't just use the criterion of agreement or disagreement with observations. They also have aesthetic principles, and a desire for an elegant, universal theory. They use these aesthetic principles to remove the cloud of trivially competing theories that necessarily surround every theory. Occam's razor is a working rule of thumb, not the ultimate answer. Cohen J., and I. Stewart (1994). The Collapse of Chaos. New York: Viking Press. 
Parallel Distributed Processing Models--A class of neurally inspired information processing models that attempt to model information processing the way it actually takes place in the brain. This model was developed because of findings that a system of neural connections appeared to be distributed in a parallel array in addition to serial pathways. As such, different types of mental processing are considered to be distributed throughout a highly complex neuronetwork. The PDP model has 3 basic principles: a.) the representation of information is distributed (not local) b.) memory and knowledge for specific things are not stored explicitly, but stored in the connections between units. c.) learning can occur with gradual changes in connection strength by experience. "These models assume that information processing takes place through interactions of large numbers of simple processing elementscalled units, each sending excitatory and inhibitory signals to other units." (McLelland, J., Rumelhart, D., & Hinton, G., 1986,p.10) Rumelhart, Hinton, and McClelland (1986) state that there are 8 major components of the PDP model framework: 1.) a set of processing units 2.) a state of activation 3.) an output function for each unit 4.) a pattern of connectivity among units 5.) a propagation rule for propagating patterns of activities through the network of connectivities 6.) an activation rule for combining the inputs impinging on a unit with the current state of that unit to produce a new level of activation for the unit 7.) a learning rule whereby patterns of connectivity are modified by experience 8.) an environment within which the system must operate. Rumelhart, D.E., Hinton, G.E., & McClelland, J.L. (1986). A General Framework for Parallel Distributed Processing. In Rumelhart, D.E., & McClelland, J.L. and the PDP Research Group (1986) Eds. Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations. MIT Press: Cambridge, MA. 
Perseveration Errors--On a recall portion of a memory task, these are errors that occur when a subject repeats items that they have already said on that same recall trial. 
Piaget's Stage Theory of Development--Piaget was among other things, a psychologist who was interested in cognitive development. After observation of many children, he posited that children progress through 4 stages and that they all do so in the same order. These four stages are described below. The Sensorimotor Period (birth to 2 years): During this time, Piaget said that a child's cognitive system is limited to motor reflexes at birth, but the child builds on these reflexes to develop more sophisicated procedures. They learn to generalize their activities to a wider range of situations and coordinate them into increasingly lengthy chains of behaviour. PreOperational Thought (2 to 6 or 7 years): At this age, according to Piaget, children acquire representational skills in the areas mental imagery, and especially language. They are very self-oriented, and have an egocentric view; that is, preoperational chldren can use these representational skills only to view the world from their own perspective. Concrete Operations (6/7 to 11/12): As opposed to Preoperational children, children in the concrete operations stage are able to take another's point of view and take into account more than one perspective simultaneously. They can also represent transformations as well as static situations. Although they can understand concrete problems, Piaget would argue that they cannot yet perform on abstract problems, and that they do not consider all of the logically possible outcomes. Formal Operations (11/12 to adult): Children who attain the formal operation stage are capable of thinking logically and abstractly. They can also reason theoretically. Piaget considered this the ultimate stage of development, and stated that although the children would still have to revise their knowledge base, their way of thinking was as powerful as it would get. It is now thought that not every child reaches the formal operation stage. Developmental psychologists also debate whether children do go through the stages in the way that Piaget postulated. Whether Piaget was correct or not, however, it is safe to say that this theory of cognitive development has had a tremendous influence on all modern developmental psychologists. Santrock, J. W. (1995). Children. Dubuque, IA: Brown & Benchmark. Siegler, R. (1991). Children's Thinking. Englewood Cliffs, NJ: Prentice-Hall. Vasta, R., Haith, M. M., & Miller, S. A. (1995). Child Psychology: The Modern Science. New York, NY: Wiley. 
Primacy Effect--The primacy effect is found when the results of a free recall task are plotted in the form of a serial position curve. Generally, this curve is U-shaped, and the primacy effect corresponds to the tail of the U on the left. This tail indicates that words presented at the start of a list of to-be-remembered items are better remembered than words presented in the middle of this list. It is called the primacy effect because these items were the ones presented first to the subject in the memory experiment. The primacy effect appears to be the result of subjects recalling items directly from a semantic memory. This is because the primacy effect can be sharply attenuated by performing manipulations that adversely affect this system -- such as using fast presentation of items (which does not permit much elaborative rehearsal to transfer memories from short-term to long-term stores), or by using list items that have similar meanings (and thereby producing semantic confusions). The primacy effect was important to cognitive science because it provided empirical evidence for the decomposition of memory into an organized set of subsystems, which is required by functional analysis. 
Priming--Priming is discussed in the context of the activation theory. It is assumed that concepts that have some relation to each other are connected in some mental network, so that if one concept is activated, then concepts related to it are also activated. Priming is a phenomenon related to this concept. It can be shown in the following example: A subject is shown the word nurse. Presumably the subject will then think of other words related to the word nurse. If the subject is then shown either the word doctor or the word butter, the subject should be able to read the former word more quickly than the latter word because " doctor" is related to " nurse" and therefore has been recently accessed, and so more familiar to the subject. The word nurse then serves to "prime" the second word, doctor. 
Primitive--A primitive is a basic building block of a system. Complex systems can be decomposed into simpler things, but primitives - by definition - cannot. To provide an example that gives a nice intuition about what a primitive is, consider teaching a child the meanings of different words. If a child asks us "What does `bachelor' mean?", we might break "bachelor down into other meanings ("`Bachelor' means that someone is a `man' who is `not married'"). However, if a child asks us "What does `red' mean?", we are not likely to do this, because it is difficult to decompose such a basic term. Instead, we are more likely to point to different things that are `red'. In this sense, `red' represents something that we might call a semantic primitive (a basic meaning), while `bachelor' does not. Primitives are important in cognitive science because of its tendency to view information processors functionally instead of physically. Because of this view, researchers use a methodology called functional analysis to decompose a complex information processor into simpler, functional components. However, if this decomposition is not stopped, the functional analysis goes on indefinitely and falls prey to Ryle's Regress. This means that the functional analysis is not explanatory. Researchers try to escape Ryle's regress by identifying a set of primitive functions which cannot be further decomposed. This set of functions is the functional architecture for cognition. 
Proposition--The proposition is a concept borrowed by cognitive psychologists from linguists and logicians. The propostion is the most basic unit of meaning in a representation. It is the smallest statement that can be judged either true or false. Anderson gives the follwoing example of a setnece divided up into its constituent propositions "Nixon gave a beautiful Cadillac to Brezhnev, who was the leader of the USSR" This sentence can be divided into three propositions: ,p> 1. "Noxon gave a Cadillac to Brezhnev." 2. "The Cadillac was beautiful." 3. "Brezhnev was the leader of the USSR." A popular view in cognitive psycyhology is that the mind is structured much like a language. In such a structure, propositions function as basic units of representation, or the building blocks of the mind. It is the content, of the propositions, the connections between propostiions, and the strength of the connections between propositions that determine the structure of mind. Anderson, J. (1990) Cognitive Psychology and its Implications. W. H. Freeman and Company. 
Protocol Analysis--Protocol analysis is one experimental method that can be used to gather intermediate state evidence concerning the procedures used by a system to compute a function. In protocol analysis, subjects are trained to think aloud as they solve a problem, and their verbal behaviour forms the basic data to be analyzed. The first step of a protocol analysis is to obtain, and then transcribe, a verbal protocol. The next step is to take the protocol and use it to infer the subject's problem space (i.e., infer the rules being used, as well as various knowledge states concerning the problem). The third step is to create a problem behaviour graph, which reflects state transitions as subjects search through the problem space in their attempt to solve the problem. Finally, the problem behavior graph is used to create a computer simulation (typically created as a production system) that will solve the problem. By comparing, in detail, the behaviour of the simulation to the verbal protocol, one can validate the assumptions that led to the program's creation. In turn, the program provides a rich description of an individual's processing steps, and transitions in knowledge,during the problem-solving process. Ericsson, K.A., & Simon, H.A. (1984). Protocol analysis: Verbal reports as data. Cambridge, MA: MIT Press. Newell, A., & Simon, H.A. (1972). Human problem solving. Englewood Cliffs, NJ: Prentice-Hall. 
Recency Effect--The recency effect is found when the results of a free recall task are plotted in the form of a serial position curve. Generally, this curve is U-shaped, and the recency effect corresponds to the tail of the U on the right. This tail indicates that words presented at the end of a list of to-be-remembered items are better remembered than words presented in the middle of this list. It is called the recency effect because these items were the ones presented most recently to the subject in the memory experiment. The recency effect appears to be the result of subjects recalling items directly from the maintenance rehearsal loop used to keep items in primary memory. In other words, it reflects short-term memory for items. This is because the recency effect can be sharply attenuated by performing manipulations that adversely affect such rehearsal -- such as delaying recall of list items with a distractor task, or by using list items that have similar sounds. The recency effect was important to cognitive science because it provided empirical evidence for the decomposition of memory into an organized set of subsystems, which is required by functional analysis. 
Recognition Recall--This is a variation of the recall portion of a memory task. The subject is not required to explicitly state the items, but instead, they must simply identify which items (from a larger group of items) were on the original list. For instance, the subject may be read a large list of items and be asked to say "YES" if the item was on the list, and say "NO" if it was not on the list. This task is slightly easier than the cued or free recall task. The answers provided by the subject fall into 4 categories: HITS: These are the responses that correctly identify items as being from the original list when they actually are. CORRECT NEGATIVES: These are the responses that correctly state an item as not being on the original list when it actually was not. MISSES: These are the responses that fail to identify a word as being from the original list when it was. FALSE POSITIVES: These are responses that incorrectly identify items as being from the original list when they were not on that list. 
Recursive decomposition--Recursive decomposition (Palmer & Kimchi, 1986) refers to the process whereby any complex informational event at one level of description can be specified more fully at a lower level of description by decomposing the event into: a number of components and processes that specifiy the relations among these components The information processing model of memory provides a good example of recursive decomposition. Model of Memory The research strategy, functional analysis, relies on the principle of recursive decomposition. Recursive decomposition should not be equated with reductionism, which is based on the assumption that the best of correct level of description is the most specific one (e.g., at the level of physics). Palmer, s. & Kimchi, R. (1986). The information approach to cognition. In T. Knapp, & L. Robertson (Eds.), Approaches to cognition. Hillsdale NJ: Erlbaum. Medin, D.L., & Ross, B.H. (1992). Cognitive psychology. Fort Worth: Harcourt Bruce Jonavich College Publishers 
Relative Complexity Evidence--One of the key goals of cognitive science is to develop theories that are strongly equivalent with respect to to-be-explained systems. This requires that evidence be collected to defend the claim that the model and the to-be-explained system are carrying out the same procedures to compute a function. One type of evidence that can be used to defend this claim is called relative complexity evidence. Imagine that someone is proposing that a Turing machine is a strongly equivalent model of how children do mental arithmetic. To collect relative complexity evidence concerning this claim, we could present a number of different addition problems to the Turing machine, and then rank order them in terms of the number of processing steps that each problem required. We could then present the same problems to a group of children, and rank order the difficulty they caused the children on the basis of reaction time taken to solve the problems. If the two systems are strongly equivalent, then we would expect the same rank-orderings to be obtained for both the Turing machine and the children. If they are not strongly equivalent (as we would expect in this example), then differen rank-orderings would emerge because different procedures are used to solve the problems. Pylyshyn, Z.W. (1984). Computation and cognition. Cambridge, MA: MIT Press. 
Retrieval--Retrieval refers to the processess through which we recover items from memory. 
Ryle's Regress--Ryle's Regress is a classic argument against cognitivist theories, and concludes that such theories cannot be scientific. The philosopher Gilbert Ryle (1949) was concerned with critiquing what he called the intellectualist legend, which required intelligent acts to be the product of the conscious application of mental rules. Ryle (p. 31) argued that the intellectualist legend results in an infinite regress of thought: "According to the legend, whenever an agent does anything intelligently, his act is preceded and steered by another internal act of considering a regulative proposition appropriate to his practical problem. [...] Must we then say that for the hero's reflections how to act to be intelligent he must first reflect how best to reflect how to act? The endlessness of this implied regress shows that the aplication of the appropriateness does not entail the occurrence of a process of considering this criterion." Variants of Ryle's Regress are commonly aimed at cognitivist theories. For instance, in order to explain the behavior of rats, Edward Tolman (e.g., 1932, 1948) found that he had to use terms that modern cognitive scientists would be very comfortable with. For instance, Tolman suggested that his rats were constructing a "cognitive map" that helped them locate reinforcers, and he used intentional terms (e.g., expectancies, purposes, meanings) to describe their behavior. This led to a famous attack on Tolman's work by Guthrie (1935, p. 172): "Signs, in Tolman's theory, occasion in the rat realization, or cognition, or judgement, or hypotheses, or abstraction, but they do not occasion action. In his concern with what goes on in the rat's mind, Tolman has neglected to predict what the rat will do. So far as the theory is concerned the rate is left buried in thought; if he gets to the food-box at the end that is his concern, not the concern of the theory." Cognitive scientists must be constantly aware of Ryle's Regress as a potential problem with their theories, and must ensure that their theories include a principled account of how the (potentially) infinite regress that emerges from functional analysis can be stopped. This is why the identification of the functional architecture is one of the fundamental goals of cognitive science. Guthrie, E.R. (1935). The psychology of learning. New York: Harper Ryle, G. (1949). The concept of mind. London: Hutchinson & Company. Tolman, E.C. (1932). Purposive behavior in animals. New York: Century Books. Tolman, E.C. (1948). Cognitive maps in rats and men. Psychological Review, 55, 189-208. 
Schema--A schema representation is a way of capturing the insight that concepts are defined by a configuration of features, and each of these features involves specifying a value the object has on some attribute. The schema represents a concept by pairing a class of attribute with a particular value, and stringing all the attributes together. They are a way of encoding regularities in categories, whether these regularities are propositional or perceptual. They are also general, rather than specific, so that they can be used in many situations. Anderson, J. R. (1990). Cognitive Psychology and its Implications. New York, NY: Freeman. 
Serial Position Curve--The serial position curve is used to plot the results of a free recall experiment. The x-axis of this curve indicates the serial position of to-be-remembered items in the list (e.g., the first item, the second item, the third item, and so on). The y-axis of this curve indicates the probability of recall for the item, which is typically obtained by averaging across a number of subjects. The serial position curve is important to cognitive science because it revealed two effects, the recency effect and the primacy effect, which were fundamentally important pieces of evidence for the functional decomposition of "memory" into an organized set of subsystems. 
Short-Term Memory--Generally cognitive psychologists divide memory into three stores: sensory store, short-term store, and long-term store. After entering the sensory store, some information proceeds into the short-term store. This short-term store is commonly refered to as short-term memory. Short-term memory has two important characteristics. First, short-term memory can contain at any one time seven, plus or minus two, "chunks" of informaton. Second, items remain in short-term memory around twenty seconds. These unique characteristics, among others, suggested to researchers that short-term memory was autonomous from sensory and long-term memory stores. Craik and Lockhart (1972) argued short-term memory was not autonomous from the other memory systems. They suggested that short-term memory and long-term memory were different manifestations of a single, underlying memory system. As an alternative to short-term memory Baddely and Hitch have propsed the concept of a working memory. As in traditional models of short-term memory, working memory is limited in the amount of information that it can store, and the length of time that it can store information. 
Strong Equivalence--Strong equivalence is a stronger condition for model validation than is weak equivalence. If two systems are strongly equivalent then 1) they compute the same function (i.e., they are weakly equivalent), 2) they use the same program to compute this function, and 3) this program is written in the same programming language (i.e., the two systems have the same functional architecture.) As far as "algorithmic" approaches to cognitive science are concerned (e.g., experimental psychology, psycholinguistics), the aim of the discipline is to generate strongly equivalent theories of people. This requires collecting evidence to support the claim that a simulation uses the same procedures to solve a problem as do human subjects, as well as evidence to support the claim that a proposed architecture is primitive. It is not surprising, then, that the search for strongly equivalent theories is a formidable (but necessary) challenge for cognitive scientists. Pylyshyn, Z.W. (1984). Computation and cognition. Cambridge, MA: MIT Press. 
Sustained Attention--Sustained attention is "the ability to direct and focus cognitive activity on specific stimuli." In order to complete any cognitively planned activity, any sequenced action, or any thought one must use sustained attention. An example is the act of reading a newspaper article. One must be able to focus on the activity of reading long enough to complete the task. Problems occur when a distraction arises. A distraction can interrupt and consequently interfere in sustained attention. DeGangi and Porges (1990) indicate there are 3 stages to sustained attention which include: attention getting, attention holding, and attention releasing. Sustained attention is important to psychologists because it is "a basic requirement for information processing." Therefore, sustained attention is important for cognitive development. When a person has difficulty sustaining attention, they often present with an accompanying inability to adapt to environmental demands or modify behaviour (including inhibition of inappropriate behaviour). DeGangi, Georgia and Porges, Stephen. (1990). Neuroscience Foundations of Human Performance. Rockville, MD: American Occupational Therapy Association Inc. 
Symbolic Architecture--Symbolic architecture refers to the classical view of the architecture of the mind. In this approach the mind is viewed as a process in which symbols are manipulated. Symbols are moved between memory stores such as long term and short term memory and are acted upon by an explicit set of rules in a particular sequence. The symbolic architecture is the manner in which memory stores are related and the set of rules applied to the system. The symbolic architecture approach has been widely applied and formed the basis of influential work such as Newell & Simon's Human Problem Solving. More recently, this approach to cognitive architecture has been challenged by the connectionist architecture approach. Collins, A. & E.E. Smith, Readings in Cognitive Science: A Perspective from Psychology and Artificial Intelligence, 1988, Morgan Kaufman Publishers Inc., San Mateo CA.
Top-Down Processing--The cognitive system is organized hierarchically. The most basic perceptual systems are located at the bottom of the hierarchy, and the most complex cogntive (e.g. memory, problem solving) systems are located at the top of the hierarchy. Information can flow both from the bottom of the system to the top of the system and from the top of the system to the bottom of the system. When information flows from the top of the sytstem to the bottom of the system this is called "top-down" processing. The implications of this top to bottom flow if information is that information coming into the system (perceptually) can be influenced by what the individual already knows about the information that is coming into the system (as information about past experiences are stored in the higher levels of the system). Extreme versions of top-down processing argue that all information coming into9 the system is affected by what is already known about the world. An alternative version is offered by Jerry Fodor (1983). In his theory of modularity, Fodor argues that top-down processing occurs only in some parts of the cognitive system at certain times. Fodor rejects the idea that all stored information can pootentially effect all incoming information. 
Turing Equivalence--Turing equivalence is another term for describing weak equivalence. 
Turing Test--The Turing test is a behavioural approach to determining whether or not a system is intelligent. It was originally proposed by mathematician Alan Turing, one of the founding figures in computing. Turing argued in a 1950 paper that conversation was the key to judging intelligence. In the Turing test, a judge has conversations (via teletype) with two systems, one human, the other a machine. The conversations can be about anything, and proceed for a set period of time (e.g., an hour). If, at the end of this time, the judge cannot distinguish the machine from the human on the basis of the conversation, then Turing argued that we would have to say that the machine was intelligent. There are a number of different views about the utility of the Turing test in cognitive science. Some researchers argue that it is the benchmark test of what Searle calls strong AI, and as a result is crucial to defining intelligence. Other researchers take the position that the Turing test is too weak to be useful in this way, because many different systems can generate correct behaviours for incorrect (i.e., unintelligent) reasons. Famous examples of this are Weizenbaum's ELIZA program and Colby's PARRY program. Indeed, the general acceptance of ELIZA as being "intelligent" so appalled Weizenbaum that he withdrew from mainstream AI research, which he attacked in his landmark 1976 book. Colby, K.M. et al. (1972) Artificial paranoia. Artificial Intelligence, 2, 1-26. Colby, K.M. et al. (1973) Turing-like undistinguishability tests for the validation of a computer simulation of paranoid processes. Artificial Intelligence, 3, 47-51. Turing, A.M. (1950). Computing machinery and intelligence. Mind, 59, 433-560. Weizenbaum, J. (1976). Computer power and human reason. San Francisco, CA: W.H. Freeman. 
Visuospatial Perception--This is one component of cognitive functioning and it refers to our ability to process and interpret visual information about where objects are in space. This is an important aspect of cognitive functioning because it is responsible for a wide range of activities of daily living. For instance, it underlies our ability to move around in an environment and orient ourselves appropriately. Visuospatial perception is also involved in our ability to accurately reach for objects in our visual field and our ability to shift our gaze to different points in space. The association areas of the visual cortex are separated into 2 major component pathways, and are believed to mediate different aspects of visual cognition. In humans, he parieto-occipital region is believed to process visuospatial and visual motion types of information. Whereas, the inferotemporal region of the brain is believed to mediate our ability to process visual information about the form and color of objects. Pinel, J. (1993). Biopsychology,(2nd Edition) Allyn & Bacon: Toronto. Kolb, B., & Whishaw, I. (1985). Fundamentals of Human Neuropsychology (2nd Edition) W.H. Freeman & Co.: New York 
Visuospatial Sketchpad--The visuospatial sketchpad or scratchpad (VSSP) is one of two passive slave systems in Baddeley's (1986) model of working memory. The VSSP is responsible for the manipulation and temporary storage of visual and spatial information. To date, more is known about the second slave system, the articulatory loop, than about visual coding in memory. Baddeley, A. (1986). Working memory. Oxford: Clarendon Press. 
WAIS--The Wechsler Adult Intelligence Scale (WAIS)was developed by Wechsler in 1955. An updated version of the scale (WAIS-R) was developed in 1981. WAIS measures global or general intelligence and is commonly used by psychologists. It is divided into 2 parts: the verbal scale and the performance scale. Each of these 2 parts is further divided into subtests each of which taps a specific verbal or nonverbal skill. Each subtest has items ranging from easy to increasingly more difficult. Verbal subtests measure "our store of knowldedge." (Belsky, 1990, p. 120) They focus on "learned or absorbed knowledge" testing "knowledge of historical, literary or biological facts; knowledge relating to competent functioning in the world; knowledge of mathematics; knowledge of the meaning of specific words." (Belsky, 1990, p. 120) Performance subtests (except picture completion) contain relatively unfamiliar tasks. They measure "on-the-spot analytical skills, how well a person can master a new, never before encountered problem." (Belsky, 1990, p. 120) Speed is critical to these tasks as these subtests are timed. The IQ measure of a person is derived by comparison to a particular reference group, to people of that test subject's age group. Therefore, the raw score has a different meaning depending upon the test subject's age. The WAIS is not only important to psychologists as a commonly used assessment tool, but it is often at the centre of the debate of whether or not intelligence declines with age. It is questionable whether the current intelligence tests (specifically the WAIS) are appropriate for use with older persons. Belsky (1990) says critics must be "looking critically at the appropriateness of the measures themselves, questioning whether existing tests of intelligence are really doing an adequate job of tapping cognitive ability in middle-aged and elderly adults." (p. 119) Belsky asks further if "the dramatic age decline is confined mainly to particular subtests. Would we see the same age loss if we looked at data other than the cross-sectional studies used to determine the norms?" (p. 121) Belsky, Janet K. (1990). The Psychology of Aging Theory, Research, and Interventions. Pacific Grove: Brooks/Cole Publishing Company. 
Weak Equivalence--Weak equivalence is a relationship between the outputs of two systems that are being compared. If these systems are only weakly equivalent, then we can say that they are computing the same function (or generating the same external behavior), but that they are using different procedures to do so. For example, human chess players and computer chess players are weakly equivalent, in the sense that they both play the game of chess, but use very different procedures to decide which move to make next in a game. (Computer chess players usually use some form of intensive search, which is beyond the memory capacity of human players. Indeed, an interesting question is how humans can play chess so well given that they do not use brute force search methods!) Weak equivalence is important in cognitive science in two respects. First, it is the kind of comparison that the Turing test offers, which is why it is also sometimes called Turing equivalence. Second, although weak equivalence is necessary for validating theories in cognitive science, it is not sufficient. This is because while it is required of theories or simulations in cognitive science that they compute the same functions as the to-be-explained system, it is also crucial that they compute these functions in the same way. This later requirement is called strong equivalence. Pylyshyn, Z.W. (1984). Computation and cognition. Cambridge, MA: MIT Press. 
Linguistic Determination--Linguistic determination is the argument that language directly effects that way that people think about and see the world. Linguistic determination is also known as the Whorfian hypothesis or the Sapir-Whorf hypothesis (Sapir, 1968; Whorf, 1956). Whorf provides the example of the Eskimo words for snow. The Eskimo people are inhabitants of the Arctic. Whereas in the English language there is only one word for snow the Eskimo language has many words for snow. Whorf argues that this language for snow allows the Eskimo people to "see" snow differently than speakers of other languages who do not have as many words for snow. That is, Eskimo people see subtle differences in snow that other people do not. Researchers have studied color perception across different linguistic groups to find support for the Whorfian hypothesis (Berlin & Kay, 1969; Heider, 1972); Heider & D. Oliver, 1973; G.A. Miller & Johnson-Laird, 1976; Rosch, 1974). The evidence indicates that people of all cultures perceive colour in the same way. The tentative conclusion is that language does not determine the way that people think. It is possible that language, while not determining the way that people think may influence the way that people think. Exactly how language might influence thought is yet unclear. 
Sapir-Whorf Hypothesis--see Linguistic Determination
Working Memory--Working memory, the more contemporary term for short-term memory, is conceptualized as an active system for temporarily storing and manipulating information needed in the execution of complex cognitive tasks (e.g., learning, reasoning, and comprehension). There are two types of components: storage and central executive functions (see Baddeley, 1986 for a review). The two storage systems within the model (the articulatory loop [AL] and the visuospatial sketchpad or scratchpad [VSSP] are seen as relatively passive slave systems primarily responsible for the temporary storage of verbal and visual information (respectively). The most important, and least understood, aspect of Working Memory is the central executive, which is conceptualized as very active and responsible for the selection, initiation, and termination of processing routines (e.g., encoding, storing, and retrieving).  Baddeley, A. (1986). Working memory. Oxford: Clarendon Press. 
Z Lens--The Z Lens is a sophisticated piece of apparatus developed by Roger Sperry and his associates in 1955 to enable them to project visual stimuli onto the retina of the eye so that they are interpreted either by the left or right hemisphere of the brain, not both at once. Sperry, a pioneer of the split brain operation, used it to demonstrate that split brain patients had two separate visual inner worlds. If the picture of an object was presented to the left hemisphere, the patient recognized it when it was presented again to the same hemisphere. However, if the same object was presented to the other half of the visual field, the patient had no recollection of having seen it before.Kristal, L. ed. (1981). ABC of Psychology. London: Multimedia Publications Inc. 